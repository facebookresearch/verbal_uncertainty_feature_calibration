{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import pickle\n",
    "current_dir = os.getcwd()\n",
    "root_path = os.path.dirname(os.path.dirname(current_dir))\n",
    "sys.path.append(f\"{root_path}/sem_uncertainty/\")\n",
    "from semantic_entropy.utils import best_split\n",
    "sys.path.append(root_path)\n",
    "from src.detection_utils import LLAMA_PROBE_PATHS\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = ['trivia_qa', 'nq_open', 'pop_qa']\n",
    "# model_name = \"Meta-Llama-3.1-8B-Instruct\"\n",
    "model_name = \"Mistral-7B-Instruct-v0.3\"\n",
    "# model_name = \"Qwen2.5-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_vu_threshold {'trivia_qa': 0.29292929292929293, 'nq_open': 0.37373737373737376, 'pop_qa': 0.43434343434343436}\n",
      "all_se_threshold {'trivia_qa': 0.9535958465934934, 'nq_open': 1.0931464582901023, 'pop_qa': 1.232697069986711}\n"
     ]
    }
   ],
   "source": [
    "# get threshold for each dataset\n",
    "all_vu_threshold, all_se_threshold = {}, {}\n",
    "for dataset in datasets:\n",
    "    # 'verbal_uncertainty' 'sentence_semantic_entropy' \"sentence_eigen\"\n",
    "    data = pd.read_csv(f\"{root_path}/datasets/{dataset}/{model_name}/train.csv\")\n",
    "    vu = np.array(data['verbal_uncertainty'])\n",
    "    se = np.array(data['sentence_semantic_entropy'])\n",
    "    vu_threshold = best_split(vu, \"\")\n",
    "    all_vu_threshold[dataset] = vu_threshold\n",
    "    se_threshold = best_split(se, \"\")\n",
    "    all_se_threshold[dataset] = se_threshold\n",
    "\n",
    "print(\"all_vu_threshold\", all_vu_threshold)\n",
    "print(\"all_se_threshold\", all_se_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "def load_data(dataset, max_alpha, use_predicted, prompt_type, str_process_layers, iti_method=2):\n",
    "    data = pd.read_csv(f\"{root_path}/datasets/{dataset}/{model_name}_sentence/test.csv\")\n",
    "    label = data['label']\n",
    "\n",
    "    if use_predicted:\n",
    "        with open(f\"{root_path}/probe/\"+LLAMA_PROBE_PATHS['verbal_uncertainty'][dataset]+f\"/{dataset}_predict_results.json\") as f:\n",
    "            uncertainty_data = json.load(f)\n",
    "            lu = uncertainty_data[\"predictions\"]\n",
    "        with open(f\"{root_path}/probe/\"+LLAMA_PROBE_PATHS['sentence_semantic_entropy'][dataset]+f\"/{dataset}_predict_results.json\") as f:\n",
    "            uncertainty_data = json.load(f)\n",
    "            se = uncertainty_data[\"predictions\"]\n",
    "    else:\n",
    "        uncertainty_data = pd.read_csv(f\"{root_path}/datasets/{dataset}/{model_name}/test.csv\")\n",
    "        lu = uncertainty_data['verbal_uncertainty']\n",
    "        se = uncertainty_data['sentence_semantic_entropy']\n",
    "    if not len(data) == len(lu) == len(se):\n",
    "        print(len(data), len(lu), len(se))\n",
    "    assert len(data) == len(lu) == len(se)\n",
    "    data['verbal_uncertainty'] = lu\n",
    "    data['sentence_semantic_entropy'] = se\n",
    "\n",
    "    \n",
    "    if use_predicted:\n",
    "        out_dir = f\"{root_path}/calibration/predicted_outputs/{dataset}/{model_name}/{prompt_type}\"\n",
    "    else:\n",
    "        out_dir = f\"{root_path}/calibration/outputs/{dataset}/{model_name}/{prompt_type}\"\n",
    "    \n",
    "    if iti_method in [0, 2]:\n",
    "        re_generate_path = f\"{out_dir}/test/with_vufi_{iti_method}_{str_process_layers}_{max_alpha}.jsonl\"\n",
    "    else:\n",
    "        re_generate_path = f\"{out_dir}/test/with_vufi_1_trivia_qa_{str_process_layers}_{max_alpha}.jsonl\"\n",
    "    # print(\"re_generate_path\", re_generate_path)\n",
    "    re_generate_data = pd.read_json(re_generate_path, lines=True)\n",
    "    assert len(re_generate_data) == len(data)\n",
    "    data['re_generate'] = re_generate_data['most_likely_answer']\n",
    "\n",
    "    data = load_lu(re_generate_path, data)\n",
    "    data = load_refusal(re_generate_path, data)\n",
    "    data = load_acc(re_generate_path, data)\n",
    "    data = load_se(re_generate_path, data)\n",
    "    detection_res = load_detection_res(dataset, model_name)\n",
    "    assert len(detection_res) == len(data)\n",
    "    data['detection_res'] = detection_res\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def load_detection_res(dataset, model_name):\n",
    "    path = f\"{root_path}/detection/LR_outputs/{dataset}/{model_name}/verbal_uncertainty_sentence_semantic_entropy.json\"\n",
    "    with open(path) as f:\n",
    "        detection_res = json.load(f)[\"y_pred\"]\n",
    "    return detection_res\n",
    "\n",
    "def load_lu(re_generate_path, data):\n",
    "    vu_path = re_generate_path.replace(\"with_vufi\", \"vu\")[:-1]\n",
    "    with open(vu_path) as f:\n",
    "        re_lu = json.load(f)\n",
    "        qs = set(data['question'])\n",
    "        if len(re_lu) != len(qs):\n",
    "            print(f\"re_lu: {len(re_lu)} data: {len(qs)}\")\n",
    "        assert len(re_lu) == len(qs)\n",
    "        for i, row in data.iterrows():\n",
    "            q = row['question']\n",
    "            if q in re_lu:\n",
    "                line = [x for x in re_lu[q] if x != -1]\n",
    "                data.at[i, 're_lu'] = np.mean(line)\n",
    "            else:\n",
    "                # alpha = 0 not regenrated\n",
    "                data.at[i, 're_lu'] = -1\n",
    "\n",
    "    most_vu_path = re_generate_path.replace(\"with_vufi\", \"vu_most_likely\")[:-1]\n",
    "    with open(most_vu_path) as f:\n",
    "        re_lu = json.load(f)\n",
    "        qs = set(data['question'])\n",
    "        if len(re_lu) != len(qs):\n",
    "            print(f\"re_lu: {len(re_lu)} data: {len(qs)}\")\n",
    "        assert len(re_lu) == len(qs)\n",
    "        for i, row in data.iterrows():\n",
    "            q = row['question']\n",
    "            if q in re_lu:\n",
    "                data.at[i, 're_vu_most_likely'] = re_lu[q]\n",
    "            else:\n",
    "                # alpha = 0 not regenrated\n",
    "                data.at[i, 're_vu_most_likely'] = -1\n",
    "    return data\n",
    "\n",
    "def load_se(re_generate_path, data):\n",
    "    se_path = re_generate_path.replace(\"with_vufi\", \"uncertainty_measures\")\n",
    "    se_path = se_path.replace(\"jsonl\", \"pkl\")\n",
    "    with open(se_path, \"rb\") as infile:\n",
    "        reselt_dict = pickle.load(infile)\n",
    "        re_se = reselt_dict['uncertainty_measures']['cluster_assignment_entropy']\n",
    "        if len(re_se) != len(data):\n",
    "            print(f\"un finished re_se: {len(re_se)} data: {len(data)}\")\n",
    "        # assert len(re_se) == len(data)\n",
    "        for i, row in data.iterrows():\n",
    "            if i < len(re_se):\n",
    "                data.at[i, 're_se'] = re_se[i]\n",
    "            else:\n",
    "                data.at[i, 're_se'] = row['sentence_semantic_entropy'] ################# un finished !!!!!!!!\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_refusal(re_generate_path, data):\n",
    "    refusal_path = re_generate_path.replace(\"with_vufi\", \"refusal\")[:-1]\n",
    "    with open(refusal_path) as f:\n",
    "        re_refusal = json.load(f)\n",
    "        qs = set(data['question'])\n",
    "        if len(re_refusal) != len(qs):\n",
    "            print(f\"re_refusal: {len(re_refusal)} data: {len(qs)}\")\n",
    "        assert len(re_refusal) == len(qs)\n",
    "        for i, row in data.iterrows():\n",
    "            q = row['question']\n",
    "            if q in re_refusal:\n",
    "                data.at[i, 're_refusal'] = int(re_refusal[q])\n",
    "            else:\n",
    "                data.at[i, 're_refusal'] = -1\n",
    "    return data\n",
    "\n",
    "def load_acc(re_generate_path, data):\n",
    "    acc_path = re_generate_path.replace(\"with_vufi\", \"acc\")[:-1]\n",
    "    with open(acc_path) as f:\n",
    "        re_acc = json.load(f)\n",
    "        if len(re_acc) != len(data):\n",
    "            print(f\"re_acc: {len(re_acc)} data: {len(data)}\")\n",
    "        # assert len(re_acc) == len(data)\n",
    "        for i, row in data.iterrows():\n",
    "            a = re_acc[str(row['id'])][0]\n",
    "            data.at[i, 're_acc'] = a\n",
    "    return data\n",
    "\n",
    "def get_filtered_uncertainty(data, vu_threshold, se_threshold):\n",
    "    lu, se, acc = [], [], []\n",
    "    for i, row in data.iterrows():\n",
    "        # if row['verbal_uncertainty'] < vu_threshold and row['sentence_semantic_entropy'] > se_threshold: # regenerate\n",
    "        if row['detection_res']:\n",
    "            if row['re_generate']:\n",
    "                lu.append(row['re_lu'])\n",
    "                se.append(row['re_se'])\n",
    "                acc.append(row['re_acc'])\n",
    "            else:\n",
    "                lu.append(row['verbal_uncertainty'])\n",
    "                se.append(row['sentence_semantic_entropy'])\n",
    "                acc.append(row['accuracy'])\n",
    "            # print(row['ling_auncertainty'], vu_threshold)\n",
    "            # print(row['sentence_semantic_entropy'], se_threshold)\n",
    "            # print(row['re_lu'])\n",
    "            # print(row['re_se'])\n",
    "        else: # original\n",
    "            lu.append(row['verbal_uncertainty'])\n",
    "            se.append(row['sentence_semantic_entropy'])\n",
    "            acc.append(row['accuracy'])\n",
    "    lu = torch.tensor(lu)\n",
    "    se = torch.tensor(se)\n",
    "    acc = torch.tensor(acc)\n",
    "    return lu, se, acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hallucination ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa: 30.2\t67.9\t1.9\n",
      "nq_open: 52.2\t41.7\t6.1\n",
      "pop_qa: 58.2\t26.4\t15.4\n"
     ]
    }
   ],
   "source": [
    "# before\n",
    "for dataset in datasets:\n",
    "    data = pd.read_csv(f\"{root_path}/datasets/{dataset}/{model_name}_sentence/test.csv\")\n",
    "    label = data['label']\n",
    "    hallu_ratio = round(np.mean(label == 'hallucinated') * 100, 2)\n",
    "    with open(f\"{root_path}/sem_uncertainty/outputs/{dataset}/sentence/{model_name}/test_refusal_rate.json\") as f:\n",
    "        refusal_ratio = round(json.load(f)[\"refusal_rate\"] * 100, 2)\n",
    "    correction = round(100-hallu_ratio-refusal_ratio, 2)\n",
    "    print(f\"{dataset}: {hallu_ratio}\\t{correction}\\t{refusal_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa\t19.7\t67.0\t6.2\n",
      "nq_open\t40.8\t39.4\t13.7\n",
      "pop_qa\t26.7\t23.9\t43.6\n"
     ]
    }
   ],
   "source": [
    "# after\n",
    "max_alpha = 0.4\n",
    "iti_method = 2\n",
    "use_predicted = False\n",
    "prompt_type = 'uncertainty'\n",
    "# str_process_layers = 'range(16,28)'\n",
    "str_process_layers = 'range(15,32)'\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    if dataset == \"trivia_qa\" and iti_method==1:\n",
    "        continue\n",
    "    vu_threshold = all_vu_threshold[dataset]\n",
    "    se_threshold = all_se_threshold[dataset]\n",
    "    data = load_data(dataset, max_alpha, use_predicted, prompt_type, str_process_layers, iti_method)\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        if row['detection_res']:\n",
    "            re_lu = row['re_vu_most_likely']\n",
    "            # if np.isnan(re_lu): # if re_lu is nan, then it is not regenerated\n",
    "            #     data.at[i, 're_label'] = row[\"label\"]\n",
    "            #     data.at[i, 're_acc'] = row[\"accuracy\"]\n",
    "            #     data.at[i, 're_refusal'] = int(row[\"refusal\"])\n",
    "            #     continue\n",
    "            re_acc = row['re_acc'] #re_lu >= vu_threshold row['re_refusal']\n",
    "            if re_lu >= vu_threshold or re_acc:\n",
    "                data.at[i, 're_label'] = 'ok'\n",
    "            else:\n",
    "                data.at[i, 're_label'] = 'hallucinated'\n",
    "\n",
    "        else: # original\n",
    "            data.at[i, 're_label'] = row[\"label\"]\n",
    "            data.at[i, 're_acc'] = row[\"accuracy\"]\n",
    "            data.at[i, 're_refusal'] = int(row[\"refusal\"])\n",
    "            \n",
    "    ratio = round(np.mean(data['re_label'] == 'hallucinated') * 100, 2)\n",
    "    correct_ratio = round(np.mean(data['re_acc']) * 100, 2)\n",
    "    correct_ratio2 = []\n",
    "    for i, row in data.iterrows():\n",
    "        if row['re_refusal']:\n",
    "            correct_ratio2.append(0)\n",
    "        else:\n",
    "            correct_ratio2.append(row['re_acc'])\n",
    "    correct_ratio2 = round(np.mean(correct_ratio2) * 100, 2)\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        if np.isnan(row['re_refusal']) or row['re_refusal']==-1:\n",
    "            data.at[i, 're_refusal'] = int(row[\"refusal\"])\n",
    "\n",
    "    refusal_ratio = round(np.mean(data['re_refusal']) * 100, 2)\n",
    "    print(f\"{dataset}\\t{ratio}\\t{correct_ratio2}\\t{refusal_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the number of sample pairs that two uncertainties disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa: 27.5\n",
      "nq_open: 46.8\n",
      "pop_qa: 50.8\n"
     ]
    }
   ],
   "source": [
    "# before\n",
    "for dataset in datasets:\n",
    "    vu_threshold = all_vu_threshold[dataset]\n",
    "    se_threshold = all_se_threshold[dataset]\n",
    "    # 'verbal_uncertainty' 'sentence_semantic_entropy' \"sentence_eigen\"\n",
    "    data = pd.read_csv(f\"{root_path}/datasets/{dataset}/{model_name}/test.csv\")\n",
    "    lu = data['verbal_uncertainty']\n",
    "    lu = torch.tensor(lu)\n",
    "    se = data['sentence_semantic_entropy']\n",
    "    se = torch.tensor(se)\n",
    "\n",
    "    assert len(lu) == len(se)\n",
    "    disagree = []\n",
    "    for i in range(len(lu)):\n",
    "        if (lu[i] > vu_threshold and se[i] > se_threshold) or (lu[i] < vu_threshold and se[i] < se_threshold):\n",
    "            disagree.append(0)\n",
    "        else:\n",
    "            disagree.append(1)\n",
    "    disagree = round(np.mean(disagree) * 100, 2)\n",
    "    print(f\"{dataset}: {disagree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa: 16.8\n",
      "nq_open: 19.8\n",
      "pop_qa: 28.5\n"
     ]
    }
   ],
   "source": [
    "# after\n",
    "for dataset in datasets:\n",
    "    if dataset == \"trivia_qa\" and iti_method==1:\n",
    "        continue\n",
    "    vu_threshold = all_vu_threshold[dataset]\n",
    "    se_threshold = all_se_threshold[dataset]\n",
    "    data = load_data(dataset, max_alpha, use_predicted, prompt_type, str_process_layers, iti_method)\n",
    "    lu, se, acc = get_filtered_uncertainty(data, vu_threshold, se_threshold)\n",
    "    disagree = []\n",
    "    for i in range(len(lu)):\n",
    "        if (lu[i] > vu_threshold and se[i] > se_threshold) or (lu[i] < vu_threshold and se[i] < se_threshold):\n",
    "            disagree.append(0)\n",
    "        else:\n",
    "            disagree.append(1)\n",
    "    disagree = round(np.mean(disagree) * 100, 2)\n",
    "    print(f\"{dataset}: {disagree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between two uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa 0.46\n",
      "nq_open 0.24\n",
      "pop_qa 0.15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in datasets:\n",
    "    # 'verbal_uncertainty' 'sentence_semantic_entropy' \"sentence_eigen\"\n",
    "    data = pd.read_csv(f\"{root_path}/datasets/{dataset}/{model_name}/test.csv\")\n",
    "    lu = data['verbal_uncertainty']\n",
    "    se = data['sentence_semantic_entropy']\n",
    "\n",
    "    assert len(lu) == len(se)\n",
    "    correlation_matrix = np.corrcoef(lu, se)\n",
    "    correlation_coefficient = correlation_matrix[0, 1]\n",
    "    print(dataset, round(correlation_coefficient, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa 0.66\n",
      "nq_open 0.58\n",
      "pop_qa 0.53\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for dataset in datasets:\n",
    "    if dataset == \"trivia_qa\" and iti_method==1:\n",
    "        continue\n",
    "    vu_threshold = all_vu_threshold[dataset]\n",
    "    se_threshold = all_se_threshold[dataset]\n",
    "    data = load_data(dataset, max_alpha, use_predicted, prompt_type, str_process_layers, iti_method)\n",
    "    lu, se, acc = get_filtered_uncertainty(data, vu_threshold, se_threshold)\n",
    "    if torch.isnan(lu).any():\n",
    "        print(\"lu has nan\")\n",
    "        mask = ~torch.isnan(lu)\n",
    "        lu = lu[mask]\n",
    "        se = se[mask]\n",
    "    correlation_matrix = np.corrcoef(lu, se)\n",
    "    correlation_coefficient = correlation_matrix[0, 1]\n",
    "    print(dataset, round(correlation_coefficient,2))\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# average LU for correct and incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa: 0.19\t0.04\n",
      "nq_open: 0.23\t0.07\n",
      "pop_qa: 0.3\t0.07\n"
     ]
    }
   ],
   "source": [
    "# before\n",
    "for dataset in datasets:\n",
    "    vu_correct, vu_incorrect = [], []\n",
    "    data = pd.read_csv(f\"{root_path}/datasets/{dataset}/{model_name}_sentence/test.csv\")\n",
    "    acc = data['accuracy']\n",
    "    data = pd.read_csv(f\"{root_path}/datasets/{dataset}/{model_name}/test.csv\")\n",
    "    lu = data['verbal_uncertainty']\n",
    "    for i in range(len(lu)):\n",
    "        if acc[i]:\n",
    "            vu_correct.append(lu[i])\n",
    "        else:\n",
    "            vu_incorrect.append(lu[i])\n",
    "    vu_correct = round(np.mean(vu_correct), 2)\n",
    "    vu_incorrect = round(np.mean(vu_incorrect), 2) # high because refusal is incorrect!!\n",
    "    print(f\"{dataset}: {vu_incorrect}\\t{vu_correct}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa: 0.38999998569488525\t0.05000000074505806\n",
      "nq_open: 0.4000000059604645\t0.10000000149011612\n",
      "pop_qa: 0.6399999856948853\t0.15000000596046448\n"
     ]
    }
   ],
   "source": [
    "# after\n",
    "for dataset in datasets:\n",
    "    if dataset == \"trivia_qa\" and iti_method==1:\n",
    "        continue\n",
    "    vu_threshold = all_vu_threshold[dataset]\n",
    "    se_threshold = all_se_threshold[dataset]\n",
    "    data = load_data(dataset, max_alpha, use_predicted, prompt_type, str_process_layers, iti_method)\n",
    "    lu, se, acc = get_filtered_uncertainty(data, vu_threshold, se_threshold)\n",
    "    vu_correct, vu_incorrect = [], []\n",
    "    for i in range(len(lu)):\n",
    "        # if lu[i] is not nan\n",
    "        if not torch.isnan(lu[i]):\n",
    "            if acc[i]:\n",
    "                vu_correct.append(lu[i])\n",
    "            else:\n",
    "                vu_incorrect.append(lu[i])\n",
    "    vu_correct = np.mean(vu_correct)\n",
    "    vu_incorrect = np.mean(vu_incorrect)\n",
    "    vu_correct = round(vu_correct, 2)\n",
    "    vu_incorrect = round(vu_incorrect, 2) # high because refusal is incorrect!!\n",
    "    print(f\"{dataset}: {vu_incorrect}\\t{vu_correct}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
