{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "import sys\n",
    "sys.path.append(\"/private/home/ziweiji/Hallu_Det/src/\")\n",
    "sys.path.append(\"/home/ziweiji/Hallu_Det/src/\")\n",
    "import binary_threshold_utils\n",
    "reload(binary_threshold_utils)\n",
    "from binary_threshold_utils import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X_train, y_train, X_val, y_val, val_refusal=None, return_all=False):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # Create and train the logistic regression model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_prob = model.predict_proba(X_val)[:,1]\n",
    "    if val_refusal:\n",
    "        y_pred = y_pred[val_refusal]\n",
    "        y_prob = y_prob[val_refusal]\n",
    "        y_val = y_val[val_refusal]\n",
    "    print(\"y_pred\", np.mean(y_pred), len(y_pred))\n",
    "    print(\"y_val\", np.mean(y_val), len(y_val))\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    # AUROC\n",
    "    auroc = roc_auc_score(y_val, y_prob, average='macro') *100\n",
    "    accuracy = accuracy_score(y_val, y_pred) *100\n",
    "    f1 = f1_score(y_val, y_pred, average='macro') *100\n",
    "    loss = log_loss(y_val, y_prob)\n",
    "\n",
    "    precision = precision_score(y_val, y_pred, average='macro') *100\n",
    "    recall = recall_score(y_val, y_pred, average='macro') *100\n",
    "\n",
    "    # print(f\"Accuracy F1 Pre Recall: {accuracy:.2f}\\t{f1:.2f}\\t{precision:.2f}\\t{recall:.2f}\")\n",
    "    if return_all:\n",
    "        return auroc, accuracy, f1, precision, recall, model, y_pred, y_prob\n",
    "    else:\n",
    "        return auroc, accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on trivia_qa\n",
      "label hallucinated rate 0.2041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred 0.043 1000\n",
      "y_val 0.199 1000\n",
      "Training on nq_open\n",
      "label hallucinated rate 0.2941\n",
      "y_pred 0.087 1000\n",
      "y_val 0.288 1000\n",
      "Training on pop_qa\n",
      "label hallucinated rate 0.1867\n",
      "y_pred 0.031 1000\n",
      "y_val 0.19 1000\n",
      "79.7116042133263\t80.80000000000001\n",
      "66.02415925405742\t70.3\n",
      "74.30116959064328\t81.10000000000001\n"
     ]
    }
   ],
   "source": [
    "prompt_type = 'sentence'\n",
    "label_name = 'label'\n",
    "filter_refusal = False\n",
    "use_predicted_test = False\n",
    "train_split = 'train'\n",
    "# model_name = \"Qwen2.5-7B-Instruct\"\n",
    "# model_name = \"Mistral-7B-Instruct-v0.3\"\n",
    "model_name = 'Meta-Llama-3.1-8B-Instruct'\n",
    "# \"Qwen2.5-7B-Instruct\"\n",
    "\n",
    "outputs = []\n",
    "FEATURES = [\n",
    "    # ['sentence_semantic_entropy'],\n",
    "    # # ['sentence_eigen'],\n",
    "    # ['ling_uncertainty'],\n",
    "    ['ling_uncertainty', f'{prompt_type}_semantic_entropy'],\n",
    "    # ['ling_uncertainty', f'{prompt_type}_eigen'],\n",
    "    ]\n",
    "    \n",
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']:\n",
    "    test_dataset = dataset\n",
    "    out_dir = f\"LR_outputs/{dataset}/{model_name}/\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f\"Training on {dataset}\")\n",
    "    for feature in FEATURES:\n",
    "        X_train, y_train = load_data(model_name, dataset, train_split, feature, label_name, prompt_type, filter_refusal=filter_refusal)\n",
    "        X_val, y_val = load_data(model_name, test_dataset, 'test', feature, label_name, prompt_type, use_predicted_test=use_predicted_test, filter_refusal=filter_refusal)\n",
    "        auroc, accuracy, f1, precision, recall, model, y_pred, y_prob = train_test(X_train, y_train, X_val, y_val, return_all=True)\n",
    "        outputs.append(f\"{auroc}\\t{accuracy}\")\n",
    "        if use_predicted_test:\n",
    "            output_path = out_dir+ \"_\".join(feature)+\"_use_predicted_test.json\"\n",
    "        else:\n",
    "            output_path = out_dir+ \"_\".join(feature)+\".json\"\n",
    "\n",
    "        with open(output_path, 'w') as f:\n",
    "            data = {\"auroc\": auroc, \"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall,\n",
    "            \"y_pred\": y_pred.tolist(), \"y_prob\": y_prob.tolist(), \"y_val\": y_val.tolist()}\n",
    "            json.dump(data, f)\n",
    "            \n",
    "for o in outputs:\n",
    "    print(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'nq_open'\n",
    "test_dataset = 'nq_open'\n",
    "prompt_type = 'sentence'\n",
    "# label_name = 'label'\n",
    "label_name = 'accuracy'\n",
    "filter_refusal = False\n",
    "train_split = 'train'\n",
    "FEATURES = [\n",
    "            ['sentence_semantic_entropy'],\n",
    "            ['sentence_eigen'],\n",
    "            # ['no_refuse_sentence_semantic_entropy'],\n",
    "            # ['no_refuse_sentence_eigen'],\n",
    "            ['word_semantic_entropy'],\n",
    "            ['word_eigen'],\n",
    "            ['ling_uncertainty'],\n",
    "            ['ling_uncertainty', f'{prompt_type}_semantic_entropy', f'{prompt_type}_eigen'],\n",
    "            ['ling_uncertainty', f'{prompt_type}_semantic_entropy'],\n",
    "            ['ling_uncertainty', f'{prompt_type}_eigen'],\n",
    "            # ['ling_uncertainty', f'no_refuse_{prompt_type}_semantic_entropy', f'{prompt_type}_eigen'],\n",
    "            # ['ling_uncertainty', f'no_refuse_{prompt_type}_semantic_entropy'],\n",
    "            # ['ling_uncertainty', f'no_refuse_{prompt_type}_eigen'],\n",
    "                ]\n",
    "for use_predicted_test in [False, True]:\n",
    "    print('use_predicted_test', use_predicted_test)\n",
    "    outputs = []\n",
    "    for feature in FEATURES:\n",
    "        X_train, y_train = load_data(model_name, dataset, train_split, feature, label_name, prompt_type, filter_refusal=filter_refusal)\n",
    "        X_val, y_val = load_data(model_name, test_dataset, 'test', feature, label_name, prompt_type, use_predicted_test=use_predicted_test, filter_refusal=filter_refusal)\n",
    "        auroc, accuracy, f1, precision, recall = train_test(X_train, y_train, X_val, y_val)\n",
    "        outputs.append(f\"{round(auroc, 2)}\\t{round(accuracy, 2)}\\t{round(f1, 2)}\\t{round(precision, 2)}\\t{round(recall, 2)}\")\n",
    "\n",
    "    for o in outputs:\n",
    "        print(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
