{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_curve, auc\n",
    "\n",
    "from importlib import reload\n",
    "import sys\n",
    "sys.path.append(\"/home/ziweiji/Hallu_Det/src/\")\n",
    "import binary_threshold_utils\n",
    "reload(binary_threshold_utils)\n",
    "from binary_threshold_utils import *\n",
    "import pickle\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASET = {\n",
    "    'trivia_qa': ['trivia_qa','IDK','nq_open', 'pop_qa'],\n",
    "    'nq_open': ['nq_open','IDK', 'trivia_qa','pop_qa'],\n",
    "    'pop_qa': ['pop_qa','IDK','nq_open', 'trivia_qa']\n",
    "}\n",
    "\n",
    "# , 'ANAH', 'ANAH-v2', 'HaluEval'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.3\t60.1\t57.33\t58.34\t60.09\n",
      "60.95\t55.8\t53.59\t55.55\t56.76\n",
      "66.27\t56.8\t56.72\t58.95\t58.65\n",
      "66.17\t64.2\t58.38\t65.51\t60.06\n",
      "79.21\t78.1\t70.19\t68.63\t73.87\n",
      "69.35\t75.1\t58.63\t59.27\t58.21\n",
      "71.17\t64.1\t58.44\t60.59\t66.96\n",
      "63.65\t64.5\t55.44\t56.26\t59.35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = \"nq_open\"\n",
    "# label = 'accuracy'\n",
    "label = 'label'\n",
    "prompt_type = 'sentence'\n",
    "filter_refusal = True\n",
    "\n",
    "\n",
    "for test_dataset in TEST_DATASET[dataset]:\n",
    "    for fetaure in [f'{prompt_type}_semantic_entropy', f'{prompt_type}_eigen']:\n",
    "        Feature, Label = load_data(dataset, \"val\", fetaure, label, prompt_type, filter_refusal=filter_refusal)\n",
    "        fpr, tpr, thresholds = roc_curve(Label, Feature)\n",
    "        AUROC = auc(fpr, tpr)\n",
    "        thresh = get_threshold(thresholds, tpr, fpr)\n",
    "        # print(\"AUROC:\", AUROC, \"thresh\", thresh)\n",
    "        # acc, f1, pre, recall = getAccuracyF1(Label, Feature, thresh, average='macro')\n",
    "        # acc = round(acc*100, 2)\n",
    "        # f1 = round(f1*100, 2)\n",
    "        # pre = round(pre*100, 2)\n",
    "        # recall = round(recall*100, 2)\n",
    "        # print(\"Training:\", acc, f1, pre, recall )\n",
    "        \n",
    "        output = []\n",
    "        try:\n",
    "            test_Feature, test_Label = load_data(test_dataset, 'test', fetaure, label, prompt_type)\n",
    "            fpr, tpr, _ = roc_curve(test_Label, test_Feature)\n",
    "            test_AUROC = round(auc(fpr, tpr) * 100, 2)\n",
    "        except:\n",
    "            print('NA')\n",
    "            continue\n",
    "        output.append(str(test_AUROC))\n",
    "        acc, f1, pre, recall = getAccuracyF1(test_Label, test_Feature, thresh, average='macro')\n",
    "        acc = round(acc*100, 2)\n",
    "        output.append(str(acc))\n",
    "        f1 = round(f1*100, 2)\n",
    "        output.append(str(f1))\n",
    "        pre = round(pre*100, 2)\n",
    "        output.append(str(pre))\n",
    "        recall = round(recall*100, 2)\n",
    "        output.append(str(recall))\n",
    "\n",
    "        print(\"\\t\".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trivia_qa\t12.66\t0.25\n",
      "nq_open\t15.6\t2.42\n",
      "pop_qa\t55.53\t2.12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']:\n",
    "    refusal_rates = []\n",
    "    for type in ['sentence', 'word']:\n",
    "        with open(f\"/home/ziweiji/Hallu_Det/sem_uncertainty/outputs/{dataset}/{type}/train_refusal_rate.json\") as f:\n",
    "            data = json.load(f)\n",
    "            refusal_rates.append(round(data['refusal_rate']*100, 2))\n",
    "    print(dataset+\"\\t\"+\"\\t\".join([str(x) for x in refusal_rates]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binary with two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"trivia_qa\"\n",
    "label = 'label'\n",
    "prompt_type = 'sentence'\n",
    "fetaure = [f'{prompt_type}_semantic_entropy', f'ling_uncertainty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Feature, Label = load_data(dataset, \"val\", fetaure, label, prompt_type)\n",
    "semantic_uncertainty = Feature[:,0]\n",
    "linguistic_uncertainty = Feature[:,1]\n",
    "# Sample data: Replace with your actual data\n",
    "# Assume df is a DataFrame with columns: 'linguistic_uncertainty', 'semantic_uncertainty', 'is_hallucinated'\n",
    "train_df = pd.DataFrame({\n",
    "    'semantic_uncertainty': semantic_uncertainty,\n",
    "    'linguistic_uncertainty': linguistic_uncertainty,\n",
    "    'is_hallucinated': Label\n",
    "})\n",
    "\n",
    "\n",
    "test_Feature, test_Label = load_data(dataset, \"test\", fetaure, label, prompt_type)\n",
    "test_df = pd.DataFrame({\n",
    "    'semantic_uncertainty': test_Feature[:,0],\n",
    "    'linguistic_uncertainty': test_Feature[:,1],\n",
    "    'is_hallucinated': test_Label\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to evaluate thresholds\n",
    "def evaluate_thresholds(linguistic_threshold, semantic_threshold, data):\n",
    "    predictions = (\n",
    "        (data['linguistic_uncertainty'] < linguistic_threshold) &\n",
    "        (data['semantic_uncertainty'] > semantic_threshold)\n",
    "    ).astype(int)\n",
    "    \n",
    "    precision = precision_score(data['is_hallucinated'], predictions, average='macro')\n",
    "    precision  = round(precision*100, 2)\n",
    "    recall = recall_score(data['is_hallucinated'], predictions, average='macro')\n",
    "    recall = round(recall*100, 2)\n",
    "    f1 = f1_score(data['is_hallucinated'], predictions, average='macro')\n",
    "    f1 = round(f1*100, 2)\n",
    "    acc = accuracy_score(data['is_hallucinated'], predictions)\n",
    "    acc = round(acc*100, 2)\n",
    "    return precision, recall, f1, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Linguistic Threshold: 0.5555555555555556\n",
      "Best Semantic Threshold: 1.5350567286626968\n",
      "Best ACC Score: 79.8\n",
      "\n",
      "Test Set Evaluation:\n",
      "Precision: 68.93\n",
      "Recall: 57.42\n",
      "ACC Score: 80.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziweiji/miniconda3/envs/detect/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Grid search over possible threshold values\n",
    "linguistic_thresholds = np.linspace(min(linguistic_uncertainty), max(linguistic_uncertainty), 10)\n",
    "semantic_thresholds = np.linspace(min(semantic_uncertainty), max(semantic_uncertainty), 10)\n",
    "\n",
    "best_acc = 0\n",
    "best_thresholds = (0, 0)\n",
    "for l_thresh in linguistic_thresholds:\n",
    "    for s_thresh in semantic_thresholds:\n",
    "        precision, recall, f1, acc = evaluate_thresholds(l_thresh, s_thresh, train_df)\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_thresholds = (l_thresh, s_thresh)\n",
    "\n",
    "print(f\"Best Linguistic Threshold: {best_thresholds[0]}\")\n",
    "print(f\"Best Semantic Threshold: {best_thresholds[1]}\")\n",
    "print(f\"Best ACC Score: {best_acc}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "precision, recall, f1, acc = evaluate_thresholds(best_thresholds[0], best_thresholds[1], test_df)\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"ACC Score: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
