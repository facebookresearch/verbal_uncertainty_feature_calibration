{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import csv\n",
    "import tqdm.auto as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since mandarjoshi/trivia_qa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'rc' at /private/home/ziweiji/.cache/huggingface/datasets/mandarjoshi___trivia_qa/rc/0.0.0/0f7faf33a3908546c6fd5b73a660e0f8ff173c2f (last modified on Thu Jan  9 16:41:23 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a777bc5b099448d5997a0e284a448b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"mandarjoshi/trivia_qa\", 'rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some repated question_id ids in the dataset, so we need to remove them\n",
    "\n",
    "train_data = data['train']\n",
    "train_df = train_data.to_pandas()\n",
    "train_df = train_df.drop_duplicates(subset='question_id')\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "train_data = train_data.shuffle(seed=42).select(range(10000))\n",
    "\n",
    "validation_data = data['validation']\n",
    "validation_df = validation_data.to_pandas()\n",
    "validation_df = validation_df.drop_duplicates(subset='question_id')\n",
    "validation_data = Dataset.from_pandas(validation_df)\n",
    "test_val_data = validation_data.shuffle(seed=42).select(range(2000))\n",
    "test_data = test_val_data.select(range(1000))\n",
    "val_data = test_val_data.select(range(1000, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(data, split):\n",
    "    all_ids = set()\n",
    "    with open(f\"sampled/{split}.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"question\", \"answer\"])\n",
    "        for i in range(len(data)):\n",
    "            id = data[i]['question_id']\n",
    "            assert id not in all_ids\n",
    "            all_ids.add(id)\n",
    "            writer.writerow([id, data[i]['question'], data[i]['answer']['aliases']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(train_data, \"train\")\n",
    "write_file(val_data, \"val\")\n",
    "write_file(test_data, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qids = {}\n",
    "# for split in ['train', 'val', 'test']:\n",
    "#     qids[split] = []\n",
    "#     with open(f\"/private/home/ziweiji/Hallu_Det/datasets/triviaqa/SEP/{split}_comprehensive.csv\")  as f:\n",
    "#         reader = csv.DictReader(f)\n",
    "#         for row in reader:\n",
    "#             qids[split].append(row['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = data['train'].filter(lambda x: x['question_id'] in qids['train'])\n",
    "# no_train_data = data['train'].filter(lambda x: x['question_id'] not in qids['train'])\n",
    "# test_data = data['validation'].filter(lambda x: x['question_id'] in qids['test'])\n",
    "# no_test_data = data['validation'].filter(lambda x: x['question_id'] not in qids['test'])\n",
    "# val_data = data['validation'].filter(lambda x: x['question_id'] in qids['val'])\n",
    "# no_val_data = data['validation'].filter(lambda x: x['question_id'] not in qids['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_file(data, split, no_data):\n",
    "#     # data remove column\n",
    "#     data = data.remove_columns(['question_source', 'entity_pages', 'search_results'])\n",
    "#     no_data = no_data.remove_columns(['question_source', 'entity_pages', 'search_results'])\n",
    "\n",
    "#     id2data = {d['question_id']: d for d in data}\n",
    "#     no_id2data = {d['question_id']: d for d in no_data}\n",
    "#     no_ids = list(no_id2data.keys())\n",
    "#     overleap = len(id2data)\n",
    "#     need_to_sample = len(qids[split]) - overleap\n",
    "#     print(overleap, need_to_sample, len(no_ids))\n",
    "#     random.seed(0)\n",
    "#     sampled_ids = random.sample(no_ids, need_to_sample)\n",
    "\n",
    "#     with open(f'/private/home/ziweiji/Hallu_Det/datasets/triviaqa/sampled/{split}.csv', 'w') as f:\n",
    "#         writer = csv.DictWriter(f, fieldnames=[\"question_id\",\"question\",\"answer\"])\n",
    "#         writer.writeheader()\n",
    "#         for qid in qids[split]:\n",
    "#             if qid in id2data:\n",
    "#                 d = id2data[qid]\n",
    "#             else:\n",
    "#                 d = no_id2data[sampled_ids.pop()]\n",
    "#             d['answer'] = d['answer']['aliases']\n",
    "#             writer.writerow(d)\n",
    "#         assert len(sampled_ids) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8713 1287 67810\n"
     ]
    }
   ],
   "source": [
    "write_file(train_data, \"train\", no_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887 113 9073\n",
      "863 137 9097\n"
     ]
    }
   ],
   "source": [
    "write_file(val_data, \"val\", no_val_data)\n",
    "write_file(test_data, \"test\", no_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
