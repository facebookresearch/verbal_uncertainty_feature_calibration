{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "root_path = os.path.dirname(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for dataset in ['IDK', 'HaluEval', 'ANAH', 'ANAH-v2']:\n",
    "# for dataset in ['trivia_qa', 'pop_qa', 'nq_open']: #\n",
    "#     for split in ['train', 'val', 'test']:\n",
    "#         path = f\"./{dataset}/sampled/{split}.csv\"\n",
    "#         if os.path.exists(path):\n",
    "#             data = pd.read_csv(path)\n",
    "            \n",
    "#             # only keep id,question,answer,\n",
    "#             data = data[['id', 'question', 'answer']]\n",
    "#             for model in ['llama-3.1-8B-grpo']:\n",
    "#                 path2 =  f\"./{dataset}/{model}/{split}.csv\"\n",
    "#                 # make sure the directory exists\n",
    "#                 os.makedirs(os.path.dirname(path2), exist_ok=True)\n",
    "#                 data.to_csv(path2, index=False) \n",
    "\n",
    "# # Mistral-7B-Instruct-v0.3', 'Qwen2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ea09fcb1764730a3af109b9497430f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6099a1a9747d4c61bb7b7f1f6b2f7ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.0514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cd6a1c7db44cb09d97b2a73e21bab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc2c1b432b548c2a2fd6506f7cead21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218edccf3a4945e382f479e7483e68f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.4143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392aa3ef2c34464e8dc96e0db1465c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff77dba69ceb474bb9d8bd94329c21b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cee4e5aa1c49f39aea556adc0e2ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal rate 0.1069\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0982edc95cb42e5ae7c35032399cf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# most likely answer\n",
    "for dataset in ['trivia_qa', 'pop_qa', 'nq_open']: #\n",
    "    for type in ['sentence']: #\n",
    "        for model in ['Meta-Llama-3.1-8B-Instruct']: #'Mistral-7B-Instruct-v0.3', 'Qwen2.5-7B-Instruct'\n",
    "            for split in ['test', 'val', 'train']: #, \n",
    "                df = pd.read_csv(f\"./{dataset}/{model}/{split}.csv\")\n",
    "                out_dir = f\"../sem_uncertainty/outputs/{dataset}/{type}/{model}/\"\n",
    "                path = f\"{out_dir}/{split}_0.1.jsonl\"\n",
    "                if os.path.exists(path):\n",
    "                    refusal_path = f\"{out_dir}/{split}_refusal_rate.json\"\n",
    "                    with open(refusal_path) as f:\n",
    "                        refusal = json.load(f)['refusal']\n",
    "                    print(\"refusal rate\", np.mean(refusal))\n",
    "                    acc_path = f\"{out_dir}/{split}_most_likely_acc.json\"\n",
    "                    with open(acc_path) as f:\n",
    "                        accuracy = json.load(f) # dict\n",
    "                    out_path = f\"./{dataset}/{model}_{type}/{split}.csv\"\n",
    "                    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "                    with jsonlines.open(path) as reader:\n",
    "                        data = list(reader)\n",
    "                        if len(data) == len(df) == len(refusal):\n",
    "                            for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                                id = str(row['id'])\n",
    "                                response =  data[i]['model answers']\n",
    "                                assert len(response) == 1\n",
    "                                response = response[0]\n",
    "\n",
    "                                if refusal[i] or accuracy[id]:\n",
    "                                    l = 'ok'\n",
    "                                else:\n",
    "                                    l = 'hallucinated'\n",
    "                                df.at[i, 'model_generated'] = response\n",
    "                                df.at[i, f'label'] = l\n",
    "                                df.at[i, 'accuracy'] = accuracy[id]\n",
    "                                df.at[i, 'refusal'] = refusal[i]\n",
    "                            columns_to_keep = ['id', 'question', 'model_generated', 'label', 'accuracy', 'refusal']\n",
    "                            df = df[columns_to_keep]\n",
    "                            df.to_csv(out_path, index=False)\n",
    "                        else:\n",
    "                            print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)} vs {len(refusal)}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in [\"HaluEval/paired_data_rate_0.5_1000\",]:\n",
    "#     path = f\"{root_path}/datasets/{dataset}/test.csv\"\n",
    "#     df = pd.read_csv(path)\n",
    "#     label = df['label']\n",
    "#     accuracy = [1.0 if l == 'ok' else 0.0 for l in label]\n",
    "#     df['accuracy'] = accuracy\n",
    "#     df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']:\n",
    "    for model in ['Mistral-7B-Instruct-v0.3', 'Qwen2.5-7B-Instruct']:\n",
    "        target_directory = f\"{root_path}/datasets/{dataset}/{model}/only_question_last_activations\" \n",
    "        link_name = f\"{root_path}/datasets/{dataset}/{model}_sentence/only_question_last_activations\"\n",
    "        os.remove(link_name)\n",
    "        os.symlink(target_directory, link_name)\n",
    "    # target_directory = f\"{root_path}/datasets/{dataset}/Meta-Llama-3.1-8B-Instruct/only_question_last_activations\" \n",
    "    # link_name = f\"{root_path}/datasets/{dataset}/Meta-Llama-3.1-8B-Instruct_sentence/only_question_last_activations\"\n",
    "    # os.remove(link_name)\n",
    "    # os.symlink(target_directory, link_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verbal_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 19666.92it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 20221.11it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 19248.40it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 18741.72it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 20170.45it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 19408.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']: #\n",
    "    for split in ['test', 'val']: # 'train', \n",
    "        for model in ['llama-3.1-8B-grpo']: #'Qwen2.5-7B-Instruct', 'Mistral-7B-Instruct-v0.3' Meta-Llama-3.1-8B-Instruct\n",
    "            input_path = f\"{dataset}/{model}/{split}.csv\"\n",
    "            df = pd.read_csv(input_path)\n",
    "            path = f\"../verbal_uncertainty/outputs_10/{model}_{dataset}_{split}_uncertainty_1.0_lu-llm-judge.json\"\n",
    "            if os.path.exists(path):\n",
    "                with open(path, \"rb\") as reader:\n",
    "                    data = json.load(reader)\n",
    "                    # assert len(data) == len(df)\n",
    "                    if len(data) == len(df):\n",
    "                        for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                            assert len(data[i]) == 10\n",
    "                            verbal_uncertainty = np.mean([x for x in data[i] if x !=-1])\n",
    "                            df.at[i, 'verbal_uncertainty'] = verbal_uncertainty\n",
    "\n",
    "                        df.to_csv(input_path, index=False)\n",
    "                    else:\n",
    "                        print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "            else:\n",
    "                print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 18377.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# trivia_qa, train 9300 vs 10000\n",
    "for dataset in ['IDK']: #\n",
    "    for split in ['test']: #\n",
    "        input_path = f\"{root_path}/datasets/{dataset}/sampled/{split}.csv\"\n",
    "        df = pd.read_csv(input_path)\n",
    "        path = f\"{root_path}/verbal_uncertainty/outputs/Meta-Llama-3.1-8B-Instruct_{dataset}_{split}_uncertainty_1.0_lu-llm-judge.p\"\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"rb\") as reader:\n",
    "                data = pickle.load(reader)\n",
    "                # assert len(data) == len(df)\n",
    "                if len(data) == len(df):\n",
    "                    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                        verbal_uncertainty = np.mean(data[i])\n",
    "                        df.at[i, 'verbal_uncertainty'] = verbal_uncertainty\n",
    "\n",
    "                    df.to_csv(input_path, index=False)\n",
    "                else:\n",
    "                    print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "        else:\n",
    "            print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for dataset in ['trivia_qa', 'nq_open', 'pop_qa']: #\n",
    "#     for split in ['train', 'val', 'test']: #\n",
    "#         input_path = f\"{root_path}/datasets/{dataset}/sampled/{split}.csv\"\n",
    "#         df = pd.read_csv(input_path)\n",
    "#         path = f\"{root_path}/sem_uncertainty/outputs/{dataset}/sentence/{split}_verbal_uncertainty.pkl\"\n",
    "#         if os.path.exists(path):\n",
    "#             with open(path, \"rb\") as reader:\n",
    "#                 data = pickle.load(reader)\n",
    "#                 # assert len(data) == len(df)\n",
    "#                 if len(data) == len(df):\n",
    "#                     for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#                         verbal_uncertainty = np.mean(data[row['id']])\n",
    "#                         df.at[i, 'natural_verbal_uncertainty'] = verbal_uncertainty\n",
    "\n",
    "#                     df.to_csv(input_path, index=False)\n",
    "#                 else:\n",
    "#                     print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "#         else:\n",
    "#             print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_predicted_uncertainty\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5262e744bb4078a152768ea77a1cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7380e1549fa4b1f9b3b7dc3aeabee4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61a481dcfc74a3ab584e329b5b7deca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed6711f966b4da888f3dc3a7a9fe1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233273a7d07e4230952d89a8831258d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cc413f93de4df48cd7019f974e44a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b7012866ff4ce48fdd4dcdc61467be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48248a22feb64b4196b42fa8cebaac8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4ef527a6eb40fa961a59e08d5b2ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5bbbceb88d4dd39071788986a48d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef227f8d4ff467d8d1f2487f2ed03ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667c407330fa432687d03f9c36c33aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4229a8332d949e498d22e323e041199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a0ff6418cd4c2794af858908d260f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21dd8ecb9934cf48c468af9fba1e808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877366f6fc354c92bf6658d8cdebbad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240fec47e4ef408fb3489bb6dc38dc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891997b7574d4fd98ec455793d32c9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']: #trivia_qa, 'nq_open', 'pop_qa'\n",
    "    for type in ['sentence']: #'no_refuse_word',\n",
    "        for model in ['Mistral-7B-Instruct-v0.3', 'Qwen2.5-7B-Instruct']:\n",
    "            for split in ['train', 'val', 'test']: \n",
    "                out_path = input_path = f\"./{dataset}/{model}/{split}.csv\"\n",
    "                df = pd.read_csv(input_path)\n",
    "                path = f\"{root_path}/sem_uncertainty/outputs/{dataset}/{type}/{model}/{split}_semantic_entropy.pkl\"\n",
    "                if os.path.exists(path):\n",
    "                    with open(path, \"rb\") as reader:\n",
    "                        data = pickle.load(reader)\n",
    "                        data = data['uncertainty_measures']['cluster_assignment_entropy']\n",
    "                        # assert len(data) == len(df)\n",
    "                        if len(data) == len(df):\n",
    "                            for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                                df.at[i, f'{type}_semantic_entropy'] = data[i]\n",
    "                            df.to_csv(out_path, index=False)\n",
    "                        else:\n",
    "                            print(f\"Length mismatch: {dataset}{model}, {split} {len(data)} vs {len(df)}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 17627.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['IDK']:\n",
    "    for type in ['no_refuse_sentence']:\n",
    "        for split in ['test']:\n",
    "            out_path = f\"{root_path}/datasets/{dataset}/sampled/{split}.csv\"\n",
    "            df = pd.read_csv(out_path)\n",
    "            path = f\"{root_path}/sem_uncertainty/outputs/{dataset}/{type}/{split}_eigen.pkl\"\n",
    "            if os.path.exists(path):\n",
    "                with open(path, \"rb\") as reader:\n",
    "                    all_results = pickle.load(reader)\n",
    "                    assert len(all_results) == len(df)\n",
    "\n",
    "                    if len(all_results) == len(df):\n",
    "                        for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                            id = row['id']\n",
    "                            if dataset == 'IDK':\n",
    "                                id = str(id)\n",
    "                            df.at[i, f'{type}_eigen'] = all_results[id][\"eigenIndicator\"]\n",
    "\n",
    "                        df.to_csv(out_path, index=False)\n",
    "                    else:\n",
    "                        print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "            else:\n",
    "                print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refusal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['semantic_ids', 'validation_is_false', 'validation_unanswerable', 'uncertainty_measures', 'alt_validation_accuracies_mean', 'alt_validation_is_false'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']:\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        out_path = f\"{root_path}/datasets/{dataset}/sampled/{split}.csv\"\n",
    "        df = pd.read_csv(out_path)\n",
    "        data_path = \"{root_path}/sem_uncertainty/outputs/pop_qa/word/test_refusal_rate.json\"\n",
    "        if os.path.exists(data_path):\n",
    "            with open(data_path) as reader:\n",
    "                data = json.load(reader)\n",
    "                all_results = data[\"refusal\"]\n",
    "                assert len(all_results) == len(df)\n",
    "\n",
    "                if len(all_results) == len(df):\n",
    "                    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                        id = row['id']\n",
    "                        df.at[i, f'if_refusal'] = all_results[id]\n",
    "                    df.to_csv(out_path, index=False)\n",
    "                else:\n",
    "                    print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "        else:\n",
    "            print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicted test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # os.walk get fall files in datastes named train.csv\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# for root, dirs, files in os.walk(\"{root_path}/datasets/\"):\n",
    "#     for file in files:\n",
    "#         if 'trivia_qa' not in root and 'nq_open' not in root and 'pop_qa' not in root:\n",
    "#             if file in [\"test.csv\", 'val.csv', 'train.csv']:\n",
    "#                 path = os.path.join(root, file)\n",
    "#                 df = pd.read_csv(path)\n",
    "#                 # rename the column answer to model_generated\n",
    "#                 df = df.rename(columns={\"answer\": \"model_generated\"})\n",
    "#                 df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
