{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['trivia_qa', 'pop_qa', 'nq_open']: #\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        path = f\"./{dataset}/sampled/{split}.csv\"\n",
    "        if os.path.exists(path):\n",
    "            data = pd.read_csv(path)\n",
    "            \n",
    "            # only keep id,question,answer,\n",
    "            data = data[['id', 'question', 'answer', 'sentence_semantic_entropy',]]\n",
    "            for model in ['Meta-Llama-3.1-8B-Instruct']:\n",
    "                path2 =  f\"./{dataset}/{model}/{split}.csv\"\n",
    "                # make sure the directory exists\n",
    "                os.makedirs(os.path.dirname(path2), exist_ok=True)\n",
    "                data.to_csv(path2, index=False) \n",
    "\n",
    "# Mistral-7B-Instruct-v0.3', 'Qwen2.5-7B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most likely answer\n",
    "for dataset in ['trivia_qa', 'pop_qa', 'nq_open']: #\n",
    "    for type in ['sentence']: #\n",
    "        for model in ['Mistral-7B-Instruct-v0.3', 'Qwen2.5-7B-Instruct']:\n",
    "            for split in ['test', 'val', 'train']: #, \n",
    "                df = pd.read_csv(f\"./{dataset}/{model}/{split}.csv\")\n",
    "                out_dir = f\"../sem_uncertainty/outputs/{dataset}/{type}/{model}/\"\n",
    "                path = f\"{out_dir}/{split}_0.1.jsonl\"\n",
    "                if os.path.exists(path):\n",
    "                    refusal_path = f\"{out_dir}/{split}_refusal_rate.json\"\n",
    "                    with open(refusal_path) as f:\n",
    "                        refusal = json.load(f)['refusal']\n",
    "                    print(\"refusal rate\", np.mean(refusal))\n",
    "                    acc_path = f\"{out_dir}/{split}_most_likely_acc.json\"\n",
    "                    with open(acc_path) as f:\n",
    "                        accuracy = json.load(f) # dict\n",
    "                    out_path = f\"./{dataset}/{model}_{type}/{split}.csv\"\n",
    "                    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "                    with jsonlines.open(path) as reader:\n",
    "                        data = list(reader)\n",
    "                        if len(data) == len(df) == len(refusal):\n",
    "                            for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                                id = str(row['id'])\n",
    "                                response =  data[i]['model answers']\n",
    "                                assert len(response) == 1\n",
    "                                response = response[0]\n",
    "\n",
    "                                if refusal[i] or accuracy[id]:\n",
    "                                    l = 'ok'\n",
    "                                else:\n",
    "                                    l = 'hallucinated'\n",
    "                                df.at[i, 'model_generated'] = response\n",
    "                                df.at[i, f'label'] = l\n",
    "                                df.at[i, 'accuracy'] = accuracy[id]\n",
    "                                df.at[i, 'refusal'] = refusal[i]\n",
    "                            columns_to_keep = ['id', 'question', 'model_generated', 'label', 'accuracy', 'refusal']\n",
    "                            df = df[columns_to_keep]\n",
    "                            df.to_csv(out_path, index=False)\n",
    "                        else:\n",
    "                            print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']:\n",
    "    for model in ['Mistral-7B-Instruct-v0.3', 'Qwen2.5-7B-Instruct']:\n",
    "        target_directory = f\"/private/home/ziweiji/Hallu_Det/datasets/{dataset}/{model}/only_question_last_activations\" \n",
    "        link_name = f\"/private/home/ziweiji/Hallu_Det/datasets/{dataset}/{model}_sentence/only_question_last_activations\"\n",
    "        os.remove(link_name)\n",
    "        os.symlink(target_directory, link_name)\n",
    "    # target_directory = f\"/private/home/ziweiji/Hallu_Det/datasets/{dataset}/Meta-Llama-3.1-8B-Instruct/only_question_last_activations\" \n",
    "    # link_name = f\"/private/home/ziweiji/Hallu_Det/datasets/{dataset}/Meta-Llama-3.1-8B-Instruct_sentence/only_question_last_activations\"\n",
    "    # os.remove(link_name)\n",
    "    # os.symlink(target_directory, link_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verbal_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']: #\n",
    "    for split in ['train']: # 'train', \n",
    "        for model in ['Meta-Llama-3.1-8B-Instruct']: #'Qwen2.5-7B-Instruct', 'Mistral-7B-Instruct-v0.3'\n",
    "            input_path = f\"{dataset}/{model}/{split}.csv\"\n",
    "            df = pd.read_csv(input_path)\n",
    "            path = f\"../verbal_uncertainty/outputs_10/{model}_{dataset}_{split}_uncertainty_1.0_lu-llm-judge.json\"\n",
    "            if os.path.exists(path):\n",
    "                with open(path, \"rb\") as reader:\n",
    "                    data = json.load(reader)\n",
    "                    # assert len(data) == len(df)\n",
    "                    if len(data) == len(df):\n",
    "                        for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                            assert len(data[i]) == 10\n",
    "                            verbal_uncertainty = np.mean([x for x in data[i] if x !=-1])\n",
    "                            df.at[i, 'verbal_uncertainty'] = verbal_uncertainty\n",
    "\n",
    "                        df.to_csv(input_path, index=False)\n",
    "                    else:\n",
    "                        print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "            else:\n",
    "                print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trivia_qa, train 9300 vs 10000\n",
    "for dataset in ['IDK']: #\n",
    "    for split in ['test']: #\n",
    "        input_path = f\"/home/ziweiji/Hallu_Det/datasets/{dataset}/sampled/{split}.csv\"\n",
    "        df = pd.read_csv(input_path)\n",
    "        path = f\"/home/ziweiji/Hallu_Det/verbal_uncertainty/outputs/Meta-Llama-3.1-8B-Instruct_{dataset}_{split}_uncertainty_1.0_lu-llm-judge.p\"\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"rb\") as reader:\n",
    "                data = pickle.load(reader)\n",
    "                # assert len(data) == len(df)\n",
    "                if len(data) == len(df):\n",
    "                    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                        verbal_uncertainty = np.mean(data[i])\n",
    "                        df.at[i, 'verbal_uncertainty'] = verbal_uncertainty\n",
    "\n",
    "                    df.to_csv(input_path, index=False)\n",
    "                else:\n",
    "                    print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "        else:\n",
    "            print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# semantic entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']: #trivia_qa, 'nq_open', 'pop_qa'\n",
    "    for type in ['sentence']: #'no_refuse_word',\n",
    "        for model in ['Mistral-7B-Instruct-v0.3', 'Qwen2.5-7B-Instruct']:\n",
    "            for split in ['train', 'val', 'test']: \n",
    "                out_path = input_path = f\"./{dataset}/{model}/{split}.csv\"\n",
    "                df = pd.read_csv(input_path)\n",
    "                path = f\"/private/home/ziweiji/Hallu_Det/sem_uncertainty/outputs/{dataset}/{type}/{model}/{split}_semantic_entropy.pkl\"\n",
    "                if os.path.exists(path):\n",
    "                    with open(path, \"rb\") as reader:\n",
    "                        data = pickle.load(reader)\n",
    "                        data = data['uncertainty_measures']['cluster_assignment_entropy']\n",
    "                        # assert len(data) == len(df)\n",
    "                        if len(data) == len(df):\n",
    "                            for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                                df.at[i, f'{type}_semantic_entropy'] = data[i]\n",
    "                            df.to_csv(out_path, index=False)\n",
    "                        else:\n",
    "                            print(f\"Length mismatch: {dataset}{model}, {split} {len(data)} vs {len(df)}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['IDK']:\n",
    "    for type in ['no_refuse_sentence']:\n",
    "        for split in ['test']:\n",
    "            out_path = f\"/home/ziweiji/Hallu_Det/datasets/{dataset}/sampled/{split}.csv\"\n",
    "            df = pd.read_csv(out_path)\n",
    "            path = f\"/home/ziweiji/Hallu_Det/sem_uncertainty/outputs/{dataset}/{type}/{split}_eigen.pkl\"\n",
    "            if os.path.exists(path):\n",
    "                with open(path, \"rb\") as reader:\n",
    "                    all_results = pickle.load(reader)\n",
    "                    assert len(all_results) == len(df)\n",
    "\n",
    "                    if len(all_results) == len(df):\n",
    "                        for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                            id = row['id']\n",
    "                            if dataset == 'IDK':\n",
    "                                id = str(id)\n",
    "                            df.at[i, f'{type}_eigen'] = all_results[id][\"eigenIndicator\"]\n",
    "\n",
    "                        df.to_csv(out_path, index=False)\n",
    "                    else:\n",
    "                        print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "            else:\n",
    "                print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refusal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "for dataset in ['trivia_qa', 'nq_open', 'pop_qa']:\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        out_path = f\"/home/ziweiji/Hallu_Det/datasets/{dataset}/sampled/{split}.csv\"\n",
    "        df = pd.read_csv(out_path)\n",
    "        data_path = \"/home/ziweiji/Hallu_Det/sem_uncertainty/outputs/pop_qa/word/test_refusal_rate.json\"\n",
    "        if os.path.exists(data_path):\n",
    "            with open(data_path) as reader:\n",
    "                data = json.load(reader)\n",
    "                all_results = data[\"refusal\"]\n",
    "                assert len(all_results) == len(df)\n",
    "\n",
    "                if len(all_results) == len(df):\n",
    "                    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "                        id = row['id']\n",
    "                        df.at[i, f'if_refusal'] = all_results[id]\n",
    "                    df.to_csv(out_path, index=False)\n",
    "                else:\n",
    "                    print(f\"Length mismatch: {dataset}, {split} {len(data)} vs {len(df)}\")\n",
    "        else:\n",
    "            print(f\"File not found: {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
